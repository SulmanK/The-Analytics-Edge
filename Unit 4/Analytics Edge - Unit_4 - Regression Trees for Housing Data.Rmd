---
title: 'Analytics Edge: Unit 4 - Regression Trees for Housing Data'
author: "Sulman Khan"
date: "October 26, 2018"
output: 
  html_document:
    css: C:/Users/Sulman/Desktop/analytics edge/gf_small_touches.css
    highlight: tango
    mathjax: default
    theme: cerulean
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
options(width = 200, scipen = 5)
options(dplyr.print_max = 200)
# options(width = 100, digits = 7)
opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE, 
               collapse = TRUE, tidy = FALSE,
               cache = TRUE, cache.path = '.cache/', 
               fig.align = 'left', dpi = 100, fig.path = 'figures/Introductiontotextanalytics/')
# opts_chunk$set(dev="png", 
#                dev.args=list(type="cairo"),
#                dpi=96)
```
## "Location, location, location!"

### Boston

* Capital of the state of Massachusetts, USA

* First settled in 1630

* 5 million people in greater Boston Area, some of the highest population densities in America

### Housing Data

* A paper was written on the relationship between **house prices** and **clean air** in the late 1970s by David Harrison of Harvard and Daniel Rubinfeld of U. of Michigan

* Data set widely used to evaluate algorithms

### The R in CART

* Trees can also be used for **regression** - the output at each leaf of the tree is no longer a category, but a number

* Just like classification trees, **regression trees** can capture **nonlinearities** that linear regression can't

### Regression Trees

* With Classification Trees we report the average outcome at each leaf of our tree, e.g. if the outcome is "true" 15 times, and "false" 5 times, the value at the leaf is : $$\frac{15}{15+5} = 0.75 \geq 0.5 -> true$$

* With Regression Trees, we have continuous variables, so we simply report the average of the values at that leaf: $$ 3,4,5 = 5$$

<center>

![](el.png)

</center>



### Housing Data

* We will explore the dataset with the aid of trees

* Compute linear regression with regression trees

* Discussing what the "cp" parameter means

* Apply cross-validation to regression trees

### Understanding the data

* Each entry to a census **tract**, a statistical division of the area that is used by researchers to break down towns and cities

* There will usually be multiple census tracts per **town**

* **LON** and **LAT** are the longitude and latitude of the center of the census tract

* **MEDV** is the median value of owner-occupied homes, in thousands of dollars

* **CRIM** is the per capita crime rate 

* **ZN** is related to how much of  the land is zoned for large residential properties 

* **INDUS** is proportion of  area used for industry

* **CHAS** is 1 if  the census tract is next to the Charles River 

* **NOX** is the concentration of  nitrous oxides in the air 

* **RM** is the average number of  rooms per dwelling 

* **AGE** is the proportion of  owner-occupied units built before 1940 

* **DIS** is a measure of  how far the tract is from centers of  employment in Boston 

* **RAD** is a measure of  closeness to important 
highways 

* **TAX** is the property tax rate per $10,000 of  value 

* **PTRATIO** is the pupil-teacher ratio by town 

### The "cp" parameter
* "cp" stands for "complexity parameter"

* Intuition: having too many splits is bad for generalization, so we should penalize the **complexity**

* Our goal when building the tree is to minimize the RSS by making splits, but we want to penalize too many splits. Define **S** to be number of splits, and lambda is the penalty. Our goal is to find the tree that minimizes:

$$\sum\limits_{Leaves} (RSS  at  each  leaf) + \lambda S$$

* lambda = 0.5

<center>

![](cp1.png)

</center>

* If we pick a large value of lambda, we won't make many splits because we pay a big price for every additional split that outweighs the decrease in "error"

* If we pick a small (or zero) value of lambda, we'll make splits until it no longer decreases error

* The definition of "cp" is closely related to lambda

* Consider a tree with no splits - we simply take the average of the data. Calculate RSS for that tree, let us call it **RSS(no splits)**

$$cp = \frac{\lambda}{RSS(no splits)}$$


# Unit 4, Recitation


### Read in data
```{r loaddata}
# Read in data
boston = read.csv("boston.csv")
# Output structure
str(boston)
```

### Plot observations
```{r plotobs}
# Plot observations
plot(boston$LON, boston$LAT)

# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)

# Plot MIT
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531],col="red", pch=20)

# Plot polution
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)

# Plot prices
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
```



### Linear Regression
```{r lat and lon}
# Linear Regression using LAT and LON
plot(boston$LAT, boston$MEDV)
plot(boston$LON, boston$MEDV)
latlonlm = lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)

# Visualize regression output
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)

latlonlm$fitted.values
points(boston$LON[latlonlm$fitted.values >= 21.2], boston$LAT[latlonlm$fitted.values >= 21.2], col="blue", pch="$")
```


### CART
```{r CART}
# Load CART packages
library(rpart)
library(rpart.plot)

# CART model
latlontree = rpart(MEDV ~ LAT + LON, data=boston)
prp(latlontree)

# Visualize output
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)

fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues>21.2], boston$LAT[fittedvalues>=21.2], col="blue", pch="$")

# Simplify tree by increasing minbucket
latlontree = rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree)
text(latlontree)

# Visualize Output
plot(boston$LON,boston$LAT)
abline(v=-71.07)
abline(h=42.21)
abline(h=42.17)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
```


### Split the data
```{r splitdata}
# Split the data
library(caTools)
set.seed(123)
split = sample.split(boston$MEDV, SplitRatio = 0.7)
train = subset(boston, split==TRUE)
test = subset(boston, split==FALSE)
```

### Linear Regression
```{r linregs}
# Create linear regression
linreg = lm(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
summary(linreg)

# Make predictions
linreg.pred = predict(linreg, newdata=test)
linreg.sse = sum((linreg.pred - test$MEDV)^2)
linreg.sse
```

### CART
```{r cartmodel}
# Create a CART model
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data=train)
prp(tree)

# Make predictions
tree.pred = predict(tree, newdata=test)
tree.sse = sum((tree.pred - test$MEDV)^2)
tree.sse
```




### Cross-Validation
```{r cvl}
# Load libraries for cross-validation
library(caret)
library(e1071)

# Number of folds
tr.control = trainControl(method = "cv", number = 10)

# cp values
cp.grid = expand.grid( .cp = (0:10)*0.001)

# What did we just do?
1*0.001 
10*0.001 
0:10
0:10 * 0.001

# Cross-validation
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)

# Extract tree
best.tree = tr$finalModel
prp(best.tree)

# Make predictions
best.tree.pred = predict(best.tree, newdata=test)
best.tree.sse = sum((best.tree.pred - test$MEDV)^2)
best.tree.sse
```


